
0. 检查系统情况：
服务器都已经安装好操作系统，交换机与服务器网卡已互联
OFED和CUDA没有强依赖关系，建议选择最新版LTS OFED驱动

1. 安装OFED
a） 下载
参见：https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/
选择LTS版本或者最新版本，下载即可
选择对应的操作系统下载，解压后执行./mlnxofedinstall
如果报错是内核版本不支持，加上--add-kernel-support 参数

b） 安装后，执行/etc/init.d/openibd restart 或重启服务器生效

c）使用
ofed_info -s
验证一下b版本是否正常
可以通过
mst start
mst status -v
看下卡是否都在

d)使用perftest验证rdma是否工作
在一台主机上运行perftest工具
Server-1:  ib_write_bw -d <ib设备> -p 17273
Server-2:  ib_write_bw -d <ib设备> <ip addr或hostname> -p 17273
其中0为cuda的设备id，也可以用—use_cuda_bus_id=<GPU full bus id>来替代
注意： 1) 这里<ib设备>用前面的mst status -v返回的mlx5_x设备替代，比如mlx5_0
       2) 确保两个主机的设备物理可通，在rail optimized优化组网的情况下，不通服务器间的mlx5_x号最好一样，用来测试同rail的组网
       3）无需为每个IB网卡分配ip地址，一个主机有一个ip地址即可

2. 安装UFM
需要单独服务器安装UFM节点（主备）
或者小集群启动OpenSM

3. CUDA，这里选择的是runfile的本地安装

a)
wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run
./cuda_12.1.1_530.30.02_linux.run
PATH=$PATH:/usr/local/cuda-12.1/bin
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.1/lib64

b）安装完cuda后，可以通过nvcc -V验证cuda工具安装成功
#此外，先安装了ofed的话，nvdia-peermem.ko也会安装，参考如下目录（取决于不同内核版本，目录有所区别）
./usr/lib/modules/4.18.0-425.3.1.el8.x86_64/kernel/drivers/video/nvidia-peermem.ko

c).加载nv peermem
modprobe nvidia_peermem
之后通过lsmod | grep peermem看下是否加载成功

4. 验证GPUdirectRDMA是否工作
a)下载安装新的perftest工具
git clone https://github.com/linux-rdma/perftest/
cd perftest
./autogen.sh
./configure CUDA_H_PATH=/usr/local/cuda/include/cuda.h
make -j
make install

b) 验证GDR是否工作
Server-1:  ib_write_bw -d <某个ib设备> -p 17273 –use-cuda=0
Server-2:  ib_write_bw -d <某个ib设备> <ip addr或者hostname> -p 17273 –use-cuda=0
其中0为cuda的设备id，也可以用—use_cuda_bus_id=<GPU full bus id>来替代，要注意网卡和GPU设备的对应关系，选择对应的mlx设备和GPU设备号

测试结果不报错即可，也可以通过这个观测服务器间网卡的带宽性能


5. 安装sharp plugin (可选）
wget https://github.com/Mellanox/nccl-rdma-sharp-plugins/archive/refs/heads/master.zip
unzip master.zip
cd nccl-rdma-sharp-plugins-master
./autogen.sh
./configure --with-cuda=/usr/local/cuda ##CUDA 安装默认目录
make
make install
#生成的库文件默认放在/usr/local/lib 下，确保NCCL在运行时能找到

6. 安装NCCL
wget https://github.com/NVIDIA/nccl/archive/refs/heads/master.zip
unzip master.zip
cd nccl-master
make -j8 src.build
make install

7. 安装NCCL test 
wget https://github.com/NVIDIA/nccl-tests/archive/refs/heads/master.zip
unzip master.zip
cd nccl-tests-master/

#找到mpi的安装目录
find / -name mpi.h
#我的返回值
./usr/mpi/gcc/openmpi-4.1.5a1/include/mpi.h

#编译nccl-test，用来测试下nccl
make MPI=1 MPI_HOME=/usr/mpi/gcc/openmpi-4.1.5a1 CUDA_HOME=/usr/local/cuda NCCL_HOME=/root/yuxi/nccl-master/build


8. 运行简单单机NCCL test测试
./all_reduce_perf -b 8 -e 8192M -f 2 -g 8

#如果报找不到mpi的动态链接库的错误，检查是否有动态库，如果没有，下载安装openmpi，执行如下命令链接动态库
vi /etc/ld.so.conf.d/mpi.conf
写入 /usr/local/lib
#然后运行
ldconfig -v

9. 多机运行nccl test，我每个服务器只有一个GPU，所以以下是个示例

mpirun --allow-run-as-root --np 2 -H 172.21.10.41,172.21.10.21  -x NCCL_IB_HCA=mlx5_0:1,mlx5_1:1 -bind-to numa \
-x NCCL_DEBUG=INFO -x CUDA_VISIBLE_DEVICES=1 ./build/all_reduce_perf -b 8 -e 8192M -f 2 -g 1

#np数和节点数对应

##########多机运行注意事项#########
#1.一定要保证服务器之间有免密钥登录
#2. 所有服务器都要安装OpenMPI, NCCL, nccl-test
#3. 保证IB网络是通的，网卡号一致



